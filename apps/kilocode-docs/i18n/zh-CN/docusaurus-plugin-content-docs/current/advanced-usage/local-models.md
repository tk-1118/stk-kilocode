# 使用本地模型

HN Code 支持使用 [Ollama](https://ollama.com/) 和 [LM Studio](https://lmstudio.ai/) 在本地计算机上运行语言模型。这提供了以下优势：

- **隐私性**：你的代码和数据永远不会离开你的计算机。
- **离线访问**：即使没有互联网连接，你也可以使用 HN Code。
- **成本节约**：避免与云端模型相关的 API 使用费用。
- **定制性**：可以尝试不同的模型和配置。

**然而，使用本地模型也有一些缺点：**

- **资源需求**：本地模型可能对资源要求较高，需要一台性能强大的计算机，最好配备专用 GPU。
- **设置复杂性**：设置本地模型可能比使用云端 API 更复杂。
- **模型性能**：本地模型的性能可能差异较大。虽然有些模型表现优秀，但可能无法与最大、最先进的云端模型相媲美。
- **功能限制**：本地模型（以及许多在线模型）通常不支持高级功能，例如提示缓存、计算机使用等。

## 支持的本地模型提供商

HN Code 目前支持两个主要的本地模型提供商：

1.  **Ollama**：一个流行的开源工具，用于在本地运行大型语言模型。它支持多种模型。
2.  **LM Studio**：一个用户友好的桌面应用程序，简化了下载、配置和运行本地模型的过程。它还提供了一个模拟 OpenAI API 的本地服务器。

## 设置本地模型

有关详细设置说明，请参阅：

- [设置 Ollama](/providers/ollama)
- [设置 LM Studio](/providers/lmstudio)

这两个提供商提供类似的功能，但用户界面和工作流程不同。Ollama 通过命令行界面提供更多控制，而 LM Studio 提供了一个更友好的图形界面。

## 故障排除

- **“由于目标计算机主动拒绝，无法建立连接”**：这通常意味着 Ollama 或 LM Studio 服务器未运行，或者 HN Code 配置的端口/地址与服务器不一致。请仔细检查 Base URL 设置。

- **响应时间慢**：本地模型可能比云端模型慢，尤其是在硬件性能较低的情况下。如果遇到性能问题，请尝试使用较小的模型。

- **找不到模型**：请确保你正确输入了模型名称。
