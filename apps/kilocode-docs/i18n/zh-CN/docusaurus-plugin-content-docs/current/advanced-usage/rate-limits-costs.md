# 速率限制与成本

了解和管理 API 使用情况对于在 HN Code 中获得流畅且经济高效的体验至关重要。本节将解释如何跟踪你的 token 使用情况、成本以及如何配置速率限制。

## Token 使用

HN Code 使用 token 与 AI 模型进行交互。Token 本质上是词语的组成部分。请求和响应中使用的 token 数量会影响处理时间和成本。

- **输入 Token：**这些是你的提示中的 token，包括系统提示、你的指令以及提供的任何上下文（例如文件内容）。
- **输出 Token：**这些是 AI 模型在响应中生成的 token。

你可以在聊天记录中查看每次交互的输入和输出 token 数量。

## 成本计算

大多数 AI 提供商根据使用的 token 数量收费。价格因提供商和具体模型而异。

HN Code 会根据配置的模型定价自动计算每次 API 请求的预估成本。该成本显示在聊天记录中，紧邻 token 使用情况。

**注意：**

- 成本计算是*预估*值。实际成本可能因提供商的计费方式而略有不同。
- 一些提供商可能提供免费层级或信用额度。请查看提供商的文档以获取详细信息。
- 一些提供商提供提示缓存，可以显著降低成本。

## 配置速率限制

为了防止意外过度使用 API 并帮助你管理成本，HN Code 允许你设置速率限制。速率限制定义了 API 请求之间的最短时间（以秒为单位）。

**如何配置：**

1.  打开 HN Code 设置（右上角的 <Codicon name="gear" /> 图标）。
2.  转到“高级设置”部分。
3.  找到“速率限制（秒）”设置。
4.  输入所需的延迟时间（以秒为单位）。值为 0 表示禁用速率限制。

**示例：**

如果你将速率限制设置为 10 秒，HN Code 会在一个 API 请求完成后至少等待 10 秒再发送下一个请求。

## 优化 Token 使用的技巧

- **简洁明了：**在提示中使用清晰简洁的语言。避免不必要的词语或细节。
- **仅提供相关上下文：**选择性使用上下文提及（`@file.ts`，`@folder/`）。仅包括与任务直接相关的文件。
- **分解任务：**将大型任务分解为更小、更专注的子任务。
- **使用自定义指令：**提供自定义指令以指导 HN Code 的行为，减少每次提示中冗长的解释。
- **选择合适的模型：**某些模型比其他模型更具成本效益。对于不需要较大模型全部功能的任务，可以考虑使用更小、更快的模型。
- **使用模式：**不同的模式可以访问不同的工具，例如 `Architect` 无法修改代码，这使其成为分析复杂代码库时的安全选择，而无需担心意外允许昂贵的操作。
- **如果未使用 MCP 则禁用它：**如果你未使用 MCP（模型上下文协议）功能，请考虑[在 MCP 设置中禁用它](/features/mcp/using-mcp-in-kilo-code#enabling-or-disabling-mcp-server-creation)，以显著减少系统提示的大小并节省 token。

通过了解和管理你的 API 使用情况，你可以高效且经济地使用 HN Code。
